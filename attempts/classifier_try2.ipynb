{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean and Standard Deviation for normalisation of images\n",
    "# Values are calculated using mean, std of images from training set (see dataset_mean_std.ipynb)\n",
    "mean = [0.4319, 0.3926, 0.3274]\n",
    "std = [0.3181, 0.2624, 0.3108]\n",
    "\n",
    "# Define individual and combo training transformation for data augmentation to artificially increase the training dataset size. \n",
    "# Including changing BCS, random rotation, random horizontal flips\n",
    "# and resize image to (224,224) and normalisation by given mean and std\n",
    "jitterTransform= transforms.Compose([\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ColorJitter(0.15, 0.5, 0.15, 0),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(torch.Tensor(mean), torch.Tensor(std)),\n",
    "    ])\n",
    "\n",
    "flipTransform= transforms.Compose([\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(torch.Tensor(mean), torch.Tensor(std)),\n",
    "    ])\n",
    "\n",
    "rotateTransform = transforms.Compose([\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.RandomRotation(25),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(torch.Tensor(mean), torch.Tensor(std)),\n",
    "    ])\n",
    "\n",
    "comboTransform1 = transforms.Compose([\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.RandomRotation(25),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(torch.Tensor(mean), torch.Tensor(std)),\n",
    "    ])\n",
    "\n",
    "comboTransform2 = transforms.Compose([\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ColorJitter(0.15, 0.5, 0.15, 0),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(torch.Tensor(mean), torch.Tensor(std)),\n",
    "    ])\n",
    "\n",
    "comboTransform3 = transforms.Compose([\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ColorJitter(0.15, 0.5, 0.15, 0),\n",
    "    transforms.RandomRotation(25),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(torch.Tensor(mean), torch.Tensor(std)),\n",
    "    ])\n",
    "\n",
    "allTransform = transforms.Compose([\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ColorJitter(0.15, 0.5, 0.15, 0),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(25),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(torch.Tensor(mean), torch.Tensor(std)),\n",
    "    ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating multiple datasets using each transform type to artificially increase dataset size\n",
    "jitterData1 = datasets.Flowers102(root='data', split = 'train', download=True, transform=jitterTransform)\n",
    "jitterData2 = datasets.Flowers102(root='data', split = 'train', download=True, transform=jitterTransform)\n",
    "jitterData3 = datasets.Flowers102(root='data', split = 'train', download=True, transform=jitterTransform)\n",
    "\n",
    "flipData1 = datasets.Flowers102(root='data', split = 'train', download=True, transform=flipTransform)\n",
    "flipData2 = datasets.Flowers102(root='data', split = 'train', download=True, transform=flipTransform)\n",
    "flipData3 = datasets.Flowers102(root='data', split = 'train', download=True, transform=flipTransform)\n",
    "\n",
    "rotateTransform1 = datasets.Flowers102(root='data', split = 'train', download=True, transform=rotateTransform)\n",
    "rotateTransform2 = datasets.Flowers102(root='data', split = 'train', download=True, transform=rotateTransform)\n",
    "rotateTransform3 = datasets.Flowers102(root='data', split = 'train', download=True, transform=rotateTransform)\n",
    "\n",
    "combo1Data1 = datasets.Flowers102(root='data', split = 'train', download=True, transform=comboTransform1)\n",
    "combo1Data2 = datasets.Flowers102(root='data', split = 'train', download=True, transform=comboTransform1)\n",
    "combo1Data3 = datasets.Flowers102(root='data', split = 'train', download=True, transform=comboTransform1)\n",
    "\n",
    "combo2Data1 = datasets.Flowers102(root='data', split = 'train', download=True, transform=comboTransform2)\n",
    "combo2Data2 = datasets.Flowers102(root='data', split = 'train', download=True, transform=comboTransform2)\n",
    "combo2Data3 = datasets.Flowers102(root='data', split = 'train', download=True, transform=comboTransform2)\n",
    "\n",
    "combo3Data1 = datasets.Flowers102(root='data', split = 'train', download=True, transform=comboTransform3)\n",
    "combo3Data2 = datasets.Flowers102(root='data', split = 'train', download=True, transform=comboTransform3)\n",
    "combo3Data3 = datasets.Flowers102(root='data', split = 'train', download=True, transform=comboTransform3)\n",
    "\n",
    "allData1 = datasets.Flowers102(root='data', split = 'train', download=True, transform=allTransform)\n",
    "allData2 = datasets.Flowers102(root='data', split = 'train', download=True, transform=allTransform)\n",
    "allData3 = datasets.Flowers102(root='data', split = 'train', download=True, transform=allTransform)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating different transformed datasets into one\n",
    "trainData = ConcatDataset([jitterData1,\n",
    "jitterData2,\n",
    "jitterData3,flipData1,flipData2,flipData3, rotateTransform1,\n",
    "rotateTransform2,\n",
    "rotateTransform3,combo1Data1,\n",
    "combo1Data2,\n",
    "combo1Data3,\n",
    "\n",
    "combo2Data1,\n",
    "combo2Data2,\n",
    "combo2Data3,\n",
    "\n",
    "combo3Data1,\n",
    "combo3Data2,\n",
    "combo3Data3,allData1,\n",
    "allData2,\n",
    "allData3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading datasets by pre-defined splits and applying defined transformations\n",
    "valTransform = transforms.Compose([\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(torch.Tensor(mean), torch.Tensor(std)),\n",
    "    ])\n",
    "\n",
    "testTransform = transforms.Compose([\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(torch.Tensor(mean), torch.Tensor(std)),\n",
    "    ])\n",
    "\n",
    "valData = datasets.Flowers102(root='data', split = 'val', download=True, transform=valTransform)\n",
    "testData = datasets.Flowers102(root='data', split = 'test', download=True, transform=testTransform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying a few images from the training dataset\n",
    "figure = plt.figure(figsize=(10, 10))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(trainData), size=(1,)).item()\n",
    "    img, label = trainData[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.permute(1,2,0), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.convStack = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Dropout(p=0.025),\n",
    "\n",
    "            nn.Conv2d(32, 64, 3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Dropout(p=0.025),\n",
    "\n",
    "            nn.Conv2d(64, 128, 3),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Dropout(p=0.025),\n",
    "\n",
    "            nn.Conv2d(128, 256, 3),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Dropout(p=0.025),\n",
    "\n",
    "            nn.Conv2d(256, 512, 3),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Dropout(p=0.025),\n",
    "\n",
    "            nn.Conv2d(512, 256, 3),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "\n",
    "        )\n",
    "        self.Flatten = nn.Flatten()\n",
    "        self.LinearStack = nn.Sequential(\n",
    "            nn.Linear(256, 102), \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convStack(x)\n",
    "        x = self.Flatten(x)\n",
    "        x = self.LinearStack(x)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 200\n",
    "LR = 0.001\n",
    "GAMMA = 0.9\n",
    "\n",
    "# Device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "# Dataloaders\n",
    "trainLoader = DataLoader(trainData, batch_size = BATCH_SIZE, shuffle = True, num_workers = 4)\n",
    "valLoader = DataLoader(valData, batch_size = BATCH_SIZE, shuffle = False, num_workers = 4)\n",
    "testLoader = DataLoader(testData, batch_size = BATCH_SIZE, shuffle = False, num_workers = 4)\n",
    "\n",
    "# Model\n",
    "model = MyCNN()\n",
    "model = model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=GAMMA, last_epoch=-1)\n",
    "\n",
    "# Loss Function\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function for model training\n",
    "losses = []\n",
    "accuracy = []\n",
    "def trainModel(dataloader, model, lossFunction, optimizer):\n",
    "    model.train()\n",
    "    currentLoss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    epochLoss = 0\n",
    "\n",
    "    for X, y in dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        total += y.size(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X)\n",
    "        loss = lossFunction(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        currentLoss += loss.item()\n",
    "        epochLoss += lossFunction(pred, y).item()\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    epochLoss = epochLoss/total\n",
    "    correct = correct/total\n",
    "\n",
    "    losses.append(epochLoss)\n",
    "    accuracy.append(correct * 100)\n",
    "    print(f'Training: Accuracy {correct * 100:>0.1f}%, Loss: {currentLoss / len(dataloader):.5f}, Epoch Loss: {epochLoss:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function to calculate validation accuracy\n",
    "def validateModel(dataloader, model, lossFunction):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    epochLoss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            total += y.size(0)\n",
    "            epochLoss += lossFunction(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    epochLoss = epochLoss/total\n",
    "    correct = (correct/total) * 100\n",
    "\n",
    "    print(f\"Validation: Accuracy: {(correct):>0.1f}%, Avg loss: {epochLoss:>8f} \\n\")\n",
    "    return epochLoss, correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying training function and output accuracy at each epoch iteration\n",
    "bestValAccuracy = 0.0\n",
    "bestValLoss = float('inf')\n",
    "bestEpoch = 0\n",
    "\n",
    "valLosses = []\n",
    "valAccuracies = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f'Epoch {epoch+1}:')\n",
    "    trainModel(trainLoader, model, loss, optimizer)\n",
    "    valLoss, valAccuracy = validateModel(valLoader, model, loss)\n",
    "    valLosses.append(valLoss)\n",
    "    valAccuracies.append(valAccuracy)\n",
    "    scheduler.step()\n",
    "\n",
    "    if (valLoss < bestValLoss):\n",
    "        bestValLoss = valLoss\n",
    "        bestValAccuracy = valAccuracy\n",
    "        bestEpoch = epoch+1\n",
    "\n",
    "print(f'Best Accuracy: {bestValAccuracy}. Best Loss: {bestValLoss} Best Epoch: {bestEpoch}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation Accuracy: 38% but a lot quicker, Learning rate 0.001"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
